{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54821e25",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "Welcome to this notebook on Naive Bayes Classification. We will cover the theory, manual calculation example, and implementation using `sklearn`. This is ideal for beginners!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1380d5",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Bayesâ€™ Theorem\n",
    "Bayesâ€™ Theorem is the foundation of the Naive Bayes Classifier:\n",
    "\n",
    "$$ P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)} $$\n",
    "\n",
    "Where:\n",
    "- $P(Y|X)$: Posterior\n",
    "- $P(X|Y)$: Likelihood\n",
    "- $P(Y)$: Prior\n",
    "- $P(X)$: Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0874bd",
   "metadata": {},
   "source": [
    "## Example Dataset (Play Tennis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1b3ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temp</th>\n",
       "      <th>PlayTennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook  Temp PlayTennis\n",
       "0     Sunny   Hot         No\n",
       "1     Sunny  Mild         No\n",
       "2  Overcast   Hot        Yes\n",
       "3     Rainy  Mild        Yes\n",
       "4     Rainy  Cool        Yes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Small dataset\n",
    "data = pd.DataFrame({\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy'],\n",
    "    'Temp': ['Hot', 'Mild', 'Hot', 'Mild', 'Cool'],\n",
    "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes']\n",
    "})\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfc359",
   "metadata": {},
   "source": [
    "## Manual Probability Calculation \n",
    "We will calculate the probability of PlayTennis = Yes/No for new input: **Outlook = Sunny**, **Temp = Cool**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d50dae",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 8) (3328918357.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"P({label}) = {prior_counts[label]}/{total}\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated f-string literal (detected at line 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count priors\n",
    "prior_counts = Counter(data['PlayTennis'])\n",
    "total = len(data)\n",
    "print(\"Prior Probabilities:\")\n",
    "for label in prior_counts:\n",
    "    print(f\"P({label}) = {prior_counts[label]}/{total} \n",
    "          = {prior_counts[label]/total:.2f}\")\n",
    "\n",
    "# Conditional probabilities with Laplace Smoothing\n",
    "def laplace_smooth(count, total, num_classes):\n",
    "    return (count + 1) / (total + num_classes)\n",
    "\n",
    "# For PlayTennis = Yes\n",
    "yes_data = data[data['PlayTennis'] == 'Yes']\n",
    "P_Sunny_Yes = laplace_smooth(len(yes_data[yes_data['Outlook'] == 'Sunny']), len(yes_data), 3)\n",
    "P_Cool_Yes = laplace_smooth(len(yes_data[yes_data['Temp'] == 'Cool']), len(yes_data), 3)\n",
    "P_Yes = prior_counts['Yes'] / total\n",
    "\n",
    "# For PlayTennis = No\n",
    "no_data = data[data['PlayTennis'] == 'No']\n",
    "P_Sunny_No = laplace_smooth(len(no_data[no_data['Outlook'] == 'Sunny']), len(no_data), 3)\n",
    "P_Cool_No = laplace_smooth(len(no_data[no_data['Temp'] == 'Cool']), len(no_data), 3)\n",
    "P_No = prior_counts['No'] / total\n",
    "\n",
    "# Compute final probabilities\n",
    "P_yes_given_X = P_Yes * P_Sunny_Yes * P_Cool_Yes\n",
    "P_no_given_X = P_No * P_Sunny_No * P_Cool_No\n",
    "\n",
    "print(f\"Posterior for Yes: {P_yes_given_X:.5f}\")\n",
    "print(f\"Posterior for No: {P_no_given_X:.5f}\")\n",
    "\n",
    "prediction = 'Yes' if P_yes_given_X > P_no_given_X else 'No'\n",
    "print(f\"ðŸ”® Predicted: PlayTennis = {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406ca9a",
   "metadata": {},
   "source": [
    "## Using `sklearn.naive_bayes`\n",
    "Now, let's use Scikit-learn to train a Naive Bayes classifier on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d16d1091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class (0=No, 1=Yes): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "# Sample data (as an example)\n",
    "data = pd.DataFrame({\n",
    "    'Outlook': ['Sunny', 'Overcast', 'Rain', 'Sunny'],\n",
    "    'Temp': ['Hot', 'Mild', 'Cool', 'Cool'],\n",
    "    'PlayTennis': ['No', 'Yes', 'Yes', 'No']\n",
    "})\n",
    "\n",
    "# Encode categorical data using separate encoders\n",
    "df_encoded = data.copy()\n",
    "\n",
    "encoders = {}\n",
    "for col in ['Outlook', 'Temp', 'PlayTennis']:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    encoders[col] = le  # store the encoder\n",
    "\n",
    "# Features and target\n",
    "X = df_encoded[['Outlook', 'Temp']]\n",
    "y = df_encoded['PlayTennis']\n",
    "\n",
    "# Train Naive Bayes model\n",
    "model = CategoricalNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "# New sample: Outlook = Sunny, Temp = Cool\n",
    "sample = pd.DataFrame({\n",
    "    'Outlook': [encoders['Outlook'].transform(['Sunny'])[0]],\n",
    "    'Temp': [encoders['Temp'].transform(['Cool'])[0]]\n",
    "})\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(sample)\n",
    "print(\"Predicted class (0=No, 1=Yes):\", pred[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b67615",
   "metadata": {},
   "source": [
    "## Variants of Naive Bayes Algorithms in `sklearn`\n",
    "Scikit-learn provides different types of Naive Bayes classifiers based on the data:\n",
    "\n",
    "- `GaussianNB` â€“ for continuous features (assumes normal distribution)\n",
    "- `MultinomialNB` â€“ for discrete count features (like word counts)\n",
    "- `BernoulliNB` â€“ for binary features (0 or 1)\n",
    "- `CategoricalNB` â€“ for categorical features (introduced in sklearn 1.0)\n",
    "\n",
    "Letâ€™s see examples of each where applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11754ffc",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "553f33d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NaÃ¯ve Bayes Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset (continuous numerical features)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Gaussian NaÃ¯ve Bayes model\n",
    "gnb = GaussianNB(priors=[0.3,0.5,0.2])\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Print Accuracy\n",
    "print(\"Gaussian NaÃ¯ve Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c3b67",
   "metadata": {},
   "source": [
    "### MultinomialNB Example (Word Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863a629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 'free offer': 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Simple spam classification example\n",
    "texts = [\"Buy cheap now\", \"Limited offer\", \"Hi friend\", \"Letâ€™s catch up\", \"Free discount\"]\n",
    "labels = [1, 1, 0, 0, 1]  # 1 = spam, 0 = ham\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(texts)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X, labels)\n",
    "print(\"Prediction for 'free offer':\", model.predict(vec.transform([\"free offer\"]))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46488b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data (emails labeled as spam=1 or not spam=0)\n",
    "text_data = [\"Buy cheap medicines online\", \"Congratulations! You won a lottery\",\n",
    "             \"Meeting at 3 PM\", \"Schedule for next week\", \"Discounts on your favorite items\"]\n",
    "labels = [1, 1, 0, 0, 1]  # 1 = Spam, 0 = Not Spam\n",
    "\n",
    "# Convert text into a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text_data)  \n",
    "\"\"\"CountVectorizer():\n",
    "This is a function from scikit-learn used to convert text into numerical feature vectors.\n",
    "It tokenizes the text (splits into words), removes punctuation, lowercases by default, \n",
    "and builds a vocabulary of all unique words in the dataset.\n",
    "It then counts how many times each word appears in each document\"\"\"\n",
    "\n",
    "# Train the Multinomial NaÃ¯ve Bayes model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X, labels)\n",
    "\n",
    "# Make predictions on a new text\n",
    "new_text = [\"Win a free iPhone now\", \"Meeting rescheduled to 5 PM\"]\n",
    "X_new = vectorizer.transform(new_text)\n",
    "predictions = mnb.predict(X_new)\n",
    "\n",
    "print(\"Predictions:\", predictions)  # Output: [1, 0] (Spam, Not Spam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2968a",
   "metadata": {},
   "source": [
    "### BernoulliNB Example (Binary Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f14bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for [1, 0, 0]: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Binary features (e.g., presence/absence)\n",
    "X = [[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 1]]\n",
    "y = [1, 1, 0, 0]  # 1 = spam, 0 = ham\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X, y)\n",
    "print(\"Prediction for [1, 0, 0]:\", bnb.predict([[1, 0, 0]])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf230af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Sample binary data (e.g., whether a customer buys a product based on three features)\n",
    "X = [[1, 0, 1], [1, 1, 0], [0, 1, 1], [1, 1, 1], [0, 0, 0]]\n",
    "y = [1, 0, 1, 1, 0]  # 1 = Buys, 0 = Doesn't buy\n",
    "\n",
    "# Train the Bernoulli NaÃ¯ve Bayes model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "new_data = [[1, 0, 0], [0, 1, 1]]\n",
    "predictions = bnb.predict(new_data)\n",
    "\n",
    "print(\"Predictions:\", predictions)  # Output: [1, 1] (Both customers buy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdc1bd",
   "metadata": {},
   "source": [
    "## Tuning Possibilities with Naive Bayes\n",
    "Although Naive Bayes has few hyperparameters, you can still explore:\n",
    "\n",
    "- **`alpha`**: Smoothing parameter in `MultinomialNB` and `BernoulliNB`\n",
    "- **`fit_prior`**: Whether to learn class priors or not\n",
    "\n",
    "You can use **GridSearchCV** or **manual tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354d19ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 0.5, 'fit_prior': True}\n",
      "Best score: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hii\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'alpha': [0.5, 1.0, 1.5], 'fit_prior': [True, False]}\n",
    "grid = GridSearchCV(MultinomialNB(), param_grid=params, cv=3)\n",
    "grid.fit(X, labels)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16093c0c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Naive Bayes is based on Bayesâ€™ Theorem.\n",
    "- It works well with small datasets and is fast.\n",
    "- Assumes features are conditionally independent.\n",
    "- Used widely in spam filtering, sentiment analysis, and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35bd92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
