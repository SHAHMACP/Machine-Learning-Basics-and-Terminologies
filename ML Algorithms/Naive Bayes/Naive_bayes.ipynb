{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V66tofGS7Vo1"
      },
      "source": [
        "## NAIVE BAYES CLASSIFICATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKqy7s7m7VpB"
      },
      "source": [
        "Naïve Bayes is a probabilistic machine learning algorithm based on Bayes' Theorem, used primarily for classification tasks. It is called \"naïve\" because it assumes that features are independent of each other, which is rarely true in real-world data but simplifies computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjdWRJnl7VpD"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxaBXR1c7VpF"
      },
      "source": [
        "### TYPES OF NAIVE BAYES CLASSIFIERS\n",
        "\n",
        "#### Gaussian Naive BAYES(GNB)\n",
        "\n",
        "* Assumes features follow a normal (Gaussian) distribution.\n",
        "\n",
        "* Used when features are continuous.\n",
        "\n",
        "* Example: Spam classification based on word frequencies.\n",
        "\n",
        "#### Multinomial naive Bayes(MNB)\n",
        "\n",
        "* Used for discrete features such as word counts in text classification.\n",
        "\n",
        "* Example: Document classification (e.g., news articles, spam detection).\n",
        "\n",
        "#### Bernoulli Naive Bayes(BNB)\n",
        "\n",
        "* Used for binary features (0/1).\n",
        "\n",
        "* Example: Sentiment analysis, where words are either present or absent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkJ60UAh7VpG"
      },
      "source": [
        "### When to Use Naïve Bayes?\n",
        "\n",
        "Fast classification: It is computationally efficient, even for large datasets.\n",
        "\n",
        "Text classification: Works well for spam filtering, sentiment analysis, and document categorization.\n",
        "\n",
        "Multiclass classification: Can handle multiple class labels efficiently.\n",
        "\n",
        "Real-time applications: Due to its simplicity and speed, it is used in real-time decision-making systems.\n",
        "\n",
        "When feature independence is reasonable: Performs best when features are not highly correlated.\n",
        "\n",
        "### When NOT to use Naïve Bayes?\n",
        "\n",
        "When features are highly dependent (e.g., complex images, deep learning tasks).\n",
        "\n",
        "If dataset size is small and class probabilities are not well-represented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "uJuIN1rp7VpI"
      },
      "source": [
        "\n",
        "### Applications of Naïve Bayes\n",
        "\n",
        "Spam Filtering – Classify emails as spam or not.\n",
        "\n",
        "Sentiment Analysis – Analyze text sentiment (positive/negative).\n",
        "\n",
        "Medical Diagnosis – Identify diseases based on symptoms.\n",
        "\n",
        "Credit Risk Prediction – Categorize loan applicants as low/high risk.\n",
        "\n",
        "Recommendation Systems – Suggest products based on user preferences.\n",
        "\n",
        "Fraud Detection – Identify fraudulent transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iunkRrSK7VpK"
      },
      "source": [
        "### Key Parameters in Naïve Bayes\n",
        "\n",
        "1. Alpha (α) - Laplace Smoothing\n",
        "\n",
        "* Used in MultinomialNB and BernoulliNB to avoid zero probabilities.\n",
        "\n",
        "* Default is α = 1, which adds a small value to word counts.\n",
        "\n",
        "2. Variance Smoothing\n",
        "\n",
        "* Used in GaussianNB to handle small variance values.\n",
        "\n",
        "* Prevents division by zero errors.\n",
        "\n",
        "3. Prior Probabilities (class_prior)\n",
        "\n",
        "* If None, the model calculates class priors from data.\n",
        "\n",
        "* If manually set, can influence classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "guVlKTBW7VpM",
        "outputId": "56b657be-2adc-45d0-f7a0-532434056230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: Spam\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Sample Dataset (Emails & Labels)\n",
        "emails = [\n",
        "    \"Win a lottery now\",         # Spam\n",
        "    \"Get cheap tickets today\",   # Spam\n",
        "    \"Lottery prize waiting for you\",  # Spam\n",
        "    \"Meeting schedule update\",   # Not Spam\n",
        "    \"Project deadline update\",   # Not Spam\n",
        "    \"Schedule your meeting now\", # Not Spam\n",
        "]\n",
        "\n",
        "labels = [1, 1, 1, 0, 0, 0]  # 1 = Spam, 0 = Not Spam\n",
        "\n",
        "# Step 2: Convert Text Data to Feature Vectors (Bag of Words)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(emails)  # Convert text to numerical features\n",
        "\n",
        "# Step 3: Split Data into Training & Testing Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Naïve Bayes Model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Test the Model on New Data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Step 7: Predict a New Email\n",
        "new_email = [\"Congratulations! You won a free lottery ticket\"]\n",
        "new_email_vectorized = vectorizer.transform(new_email)\n",
        "prediction = model.predict(new_email_vectorized)\n",
        "\n",
        "print(f\"Predicted Class: {'Spam' if prediction[0] == 1 else 'Not Spam'}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}