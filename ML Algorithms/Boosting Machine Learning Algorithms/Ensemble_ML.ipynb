{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fdb7cec",
      "metadata": {
        "id": "2fdb7cec"
      },
      "source": [
        "# Ensemble Machine Learning Algorithms   \n",
        "\n",
        "## Table of Contents\n",
        "1. Introduction to Ensemble Learning\n",
        "2. Why Ensemble Methods Work\n",
        "3. Types of Ensemble Methods\n",
        "4. Bagging Methods\n",
        "5. Boosting Methods\n",
        "6. Stacking\n",
        "7. Voting Classifiers\n",
        "8. Practical Implementation with Code\n",
        "9. When to Use Which Method\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction to Ensemble Learning\n",
        "\n",
        "**What is Ensemble Learning?**\n",
        "Ensemble learning is a machine learning technique that combines multiple models (called \"base learners\" or \"weak learners\") to create a stronger, more accurate predictive model. Think of it like asking multiple experts for their opinion and combining their answers to get a better decision.\n",
        "\n",
        "**Key Terminology:**\n",
        "- **Base Learner/Weak Learner**: Individual model in the ensemble (e.g., a single decision tree)\n",
        "- **Strong Learner**: The combined ensemble model\n",
        "- **Homogeneous Ensemble**: All base learners are of the same type\n",
        "- **Heterogeneous Ensemble**: Base learners are of different types\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why Ensemble Methods Work\n",
        "\n",
        "Ensemble methods work because they reduce three types of errors:\n",
        "\n",
        "1. **Bias**: Error from overly simplistic assumptions\n",
        "2. **Variance**: Error from sensitivity to training data variations\n",
        "3. **Noise**: Random errors in data\n",
        "\n",
        "**The Wisdom of Crowds Principle:**\n",
        "If you have multiple models making independent predictions, their errors often cancel out, leading to better overall accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Types of Ensemble Methods\n",
        "\n",
        "There are three main categories:\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating)**: Train models in parallel on different subsets of data\n",
        "2. **Boosting**: Train models sequentially, each correcting previous model's errors\n",
        "3. **Stacking**: Combine different types of models using a meta-learner\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Bagging Methods\n",
        "\n",
        "### 4.1 Concept\n",
        "Bagging creates multiple subsets of training data by random sampling with replacement (bootstrapping), trains a model on each subset, and combines predictions by voting (classification) or averaging (regression).\n",
        "\n",
        "**Key Benefits:**\n",
        "- Reduces variance\n",
        "- Helps prevent overfitting\n",
        "- Works well with high-variance models like decision trees\n",
        "\n",
        "### 4.2 Random Forest\n",
        "\n",
        "Random Forest is the most popular bagging algorithm. It builds multiple decision trees and combines their predictions.\n",
        "\n",
        "**How it Works:**\n",
        "1. Create multiple bootstrap samples from training data\n",
        "2. For each sample, build a decision tree\n",
        "3. At each split, consider only a random subset of features\n",
        "4. Combine predictions by majority voting (classification) or averaging (regression)\n",
        "\n",
        "**Important Parameters:**\n",
        "- `n_estimators`: Number of trees in the forest\n",
        "- `max_depth`: Maximum depth of each tree\n",
        "- `max_features`: Number of features to consider at each split\n",
        "- `bootstrap`: Whether to use bootstrap samples\n",
        "\n",
        "**Code Example - Random Forest Classification:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b763bb0",
      "metadata": {
        "id": "7b763bb0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
        "y = iris.target  # Target: species (0, 1, 2)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,      # Number of trees\n",
        "    max_depth=3,           # Maximum depth of trees\n",
        "    random_state=42,\n",
        "    n_jobs=-1              # Use all CPU cores\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = rf_classifier.feature_importances_\n",
        "print(\"Feature Importance:\")\n",
        "for i, importance in enumerate(feature_importance):\n",
        "    print(f\"{iris.feature_names[i]}: {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8ffde2",
      "metadata": {
        "id": "ad8ffde2"
      },
      "source": [
        "\n",
        "\n",
        "**Code Example - Random Forest Regression:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07b2ddc",
      "metadata": {
        "id": "d07b2ddc"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Random Forest Regression MSE: {mse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc61a4e",
      "metadata": {
        "id": "8bc61a4e"
      },
      "source": [
        "\n",
        "\n",
        "### 4.3 Bagging Classifier\n",
        "\n",
        "A general bagging implementation that can work with any base estimator.\n",
        "\n",
        "**Code Example:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da3f62e4",
      "metadata": {
        "id": "da3f62e4"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,           # Number of base estimators\n",
        "    max_samples=0.8,           # 80% of samples for each estimator\n",
        "    max_features=0.8,          # 80% of features for each estimator\n",
        "    bootstrap=True,            # Sample with replacement\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412eb30b",
      "metadata": {
        "id": "412eb30b"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Boosting Methods\n",
        "\n",
        "### 5.1 Concept\n",
        "\n",
        "Boosting trains models sequentially, where each new model focuses on correcting the errors made by previous models. Models are weighted based on their performance.\n",
        "\n",
        "**Key Benefits:**\n",
        "- Reduces bias and variance\n",
        "- Often achieves higher accuracy than bagging\n",
        "- Good for weak learners\n",
        "\n",
        "**Key Difference from Bagging:**\n",
        "- Bagging: Parallel training, independent models\n",
        "- Boosting: Sequential training, dependent models\n",
        "\n",
        "### 5.2 AdaBoost (Adaptive Boosting)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8902b9",
      "metadata": {
        "id": "5e8902b9"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create AdaBoost Classifier\n",
        "adaboost_clf = AdaBoostClassifier(\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
        "\n",
        "# Feature importance\n",
        "print(\"Top 5 Important Features:\")\n",
        "feature_importance = adaboost_clf.feature_importances_\n",
        "indices = np.argsort(feature_importance)[-5:]\n",
        "for i in indices:\n",
        "    print(f\"{cancer.feature_names[i]}: {feature_importance[i]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d356ea",
      "metadata": {
        "id": "08d356ea"
      },
      "source": [
        "\n",
        "\n",
        "### 5.3 Gradient Boosting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628bd93d",
      "metadata": {
        "id": "628bd93d"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "\n",
        "# Classification Example\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "gb_classifier = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb_classifier.fit(X_train_iris, y_train_iris)\n",
        "y_pred_gb = gb_classifier.predict(X_test_iris)\n",
        "\n",
        "print(f\"Gradient Boosting Classification Accuracy: {accuracy_score(y_test_iris, y_pred_gb):.4f}\")\n",
        "\n",
        "# Regression Example\n",
        "gb_regressor = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "gb_regressor.fit(X_train, y_train)  # Using diabetes data from earlier\n",
        "y_pred_gb_reg = gb_regressor.predict(X_test)\n",
        "print(f\"Gradient Boosting Regression R² Score: {r2_score(y_test, y_pred_gb_reg):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246bea09",
      "metadata": {
        "id": "246bea09"
      },
      "source": [
        "\n",
        "\n",
        "### 5.4 XGBoost (Extreme Gradient Boosting)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840402f2",
      "metadata": {
        "id": "840402f2"
      },
      "outputs": [],
      "source": [
        "# Note: Install xgboost first: pip install xgboost\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "\n",
        "    # Create XGBoost Classifier\n",
        "    xgb_classifier = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=3,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    xgb_classifier.fit(X_train_iris, y_train_iris)\n",
        "    y_pred_xgb = xgb_classifier.predict(X_test_iris)\n",
        "\n",
        "    print(f\"XGBoost Accuracy: {accuracy_score(y_test_iris, y_pred_xgb):.4f}\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"XGBoost not installed. Install with: pip install xgboost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f350927b",
      "metadata": {
        "id": "f350927b"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Stacking (Stacked Generalization)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c9da2a2",
      "metadata": {
        "id": "6c9da2a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load data\n",
        "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define base learners\n",
        "base_learners = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "]\n",
        "\n",
        "# Define meta-learner\n",
        "meta_learner = LogisticRegression()\n",
        "\n",
        "# Create Stacking Classifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=meta_learner,\n",
        "    cv=5  # Cross-validation folds\n",
        ")\n",
        "\n",
        "# Train\n",
        "stacking_clf.fit(X_train_wine, y_train_wine)\n",
        "\n",
        "# Predict\n",
        "y_pred_stack = stacking_clf.predict(X_test_wine)\n",
        "\n",
        "# Evaluate\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_score(y_test_wine, y_pred_stack):.4f}\")\n",
        "\n",
        "# Compare with individual models\n",
        "for name, model in base_learners:\n",
        "    model.fit(X_train_wine, y_train_wine)\n",
        "    y_pred_individual = model.predict(X_test_wine)\n",
        "    print(f\"{name} Accuracy: {accuracy_score(y_test_wine, y_pred_individual):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0253d1e1",
      "metadata": {
        "id": "0253d1e1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Voting Classifiers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67591a88",
      "metadata": {
        "id": "67591a88"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create individual classifiers\n",
        "clf1 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "clf3 = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Hard Voting\n",
        "hard_voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "hard_voting_clf.fit(X_train_wine, y_train_wine)\n",
        "y_pred_hard = hard_voting_clf.predict(X_test_wine)\n",
        "print(f\"Hard Voting Accuracy: {accuracy_score(y_test_wine, y_pred_hard):.4f}\")\n",
        "\n",
        "# Soft Voting\n",
        "soft_voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "soft_voting_clf.fit(X_train_wine, y_train_wine)\n",
        "y_pred_soft = soft_voting_clf.predict(X_test_wine)\n",
        "print(f\"Soft Voting Accuracy: {accuracy_score(y_test_wine, y_pred_soft):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144d2fd1",
      "metadata": {
        "id": "144d2fd1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9. When to Use Which Method\n",
        "\n",
        "### Decision Guide:\n",
        "\n",
        "**Use Random Forest when:**\n",
        "- You have high-variance data\n",
        "- You need feature importance\n",
        "- You want a robust, easy-to-use model\n",
        "- You don't have time for extensive tuning\n",
        "\n",
        "**Use AdaBoost when:**\n",
        "- You have weak learners that are slightly better than random\n",
        "- Your data has some noise but not too much\n",
        "- You need interpretability\n",
        "\n",
        "**Use Gradient Boosting when:**\n",
        "- You want maximum accuracy\n",
        "- You can afford longer training time\n",
        "- You need to tune hyperparameters carefully\n",
        "- Your data is relatively clean\n",
        "\n",
        "**Use Bagging when:**\n",
        "- You have an unstable model (high variance)\n",
        "- You want to reduce overfitting\n",
        "- You can train models in parallel\n",
        "\n",
        "**Use Stacking when:**\n",
        "- You want to combine different types of models\n",
        "- You have time for complex training\n",
        "- You need the absolute best performance\n",
        "\n",
        "**Use Voting when:**\n",
        "- You want a simple ensemble\n",
        "- You have multiple good models\n",
        "- You want interpretability\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}